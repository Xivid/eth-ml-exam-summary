% -*- root: Main.tex -*-
\section{Gaussian Processes}
% A GP $\{X_t\}_t$ is a collection of random variables from which any finite sample has a joint Gaussian distribution.\\
% For any finite set of points $T=\{t_1, \dots, t_n\}$ from a GP, it hold that $(X_{t_1}, \dots,X_{t_n})\sim \mathcal{N}(\pmb{\mu_T},\pmb{\Sigma_T})$ with $\pmb{\mu_T} = (\mu(t_1),\dots,\mu(t_n))$, $\pmb{\Sigma_T}(i,j)=k(X_{t_i},X_{t_j})$
\subsection*{Gaussian Process}
$p\big(\begin{bmatrix}
\mathbf{y} \\
y^*\\
\end{bmatrix}|x^*,\mathbf{X}, \sigma \big) = \mathcal{N}\big(\begin{bmatrix}
\mathbf{y} \\
y^*\\
\end{bmatrix} | \mathbf{0},\begin{bmatrix}
\mathbf{C_n} & \mathbf{k} \\
\mathbf{k}^T & c 
\end{bmatrix}  \big)$\\
with $\mathbf{C_n} = \mathbf{K} + \sigma^2 \mathbf{I}, c = k(x_{n+1},x_{n+1}) + \sigma^2,\\
\mathbf{k}=k(x_{n+1}, \mathbf{X}), \mathbf{K}=k(\mathbf{X}, \mathbf{X})$\\
$p(y^*|x^*, X, y) = \mathcal{N}(y^*|\mu, \sigma^2)$\\
with $\mu = k^T C_n^{-1}y, \sigma^2 = c-k^TC_n^{-1}k$

\subsection*{GP Hyperparameter Optimization}
Log-likelihood:\\
$l(Y|\theta) = -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log |C_n| - \frac{1}{2} Y^T C_n^{-1}Y$\\
Set of hyperparameters $\theta$ determine parameters $C_n$. Gradient descent: $\nabla_{\theta_i}l(Y|\theta) = -\frac{1}{2}tr(C_n^{-1} \frac{\partial C_n}{\partial \theta_i}) + \frac{1}{2} Y^T C_n^{-1} \frac{\partial C_n}{\partial \theta_i} C_n^{-1} Y$

\subsection*{Kernels}

% \subsubsection*{Polynomial kernel}
% $k_1 = (x^Ty)^m$ represents monomial of deg m \\
% $k_2 = (1+x^Ty)^m$ represents monomials up to deg m




%\subsubsection*{Properties of kernel}
%\begin{inparaitem}
%	\item k must be symmetric\\
%	\item the kernel matrix must be SPD
%\end{inparaitem}

%\subsubsection*{Kernel matrix}
%The kernel matrix $K$ is SPD \\
%$K = 
%\begin{bmatrix}
%k(x_1,x_1) & \dots & k(x_1,x_n) \\
%\vdots & \ddots & \vdots \\
%k(x_n, x_1) & \dots & k(x_n,x_n)
%\end{bmatrix}$\\
%$\left ( XX^T \right )$ for inner product as kernel.

% \subsubsection*{semi-positive-definite matrices}
% $M \in \mathbb{R}^{n\times n}$ is SPD $\Leftrightarrow$\\
% $\forall x \in \mathbb{R}^n: x^TMx \geq 0 \Leftrightarrow$\\
% all eigenvalues of $M$ are positive $\geq 0$

%\subsection*{Nearest Neighbor k-NN}
%$y=sign(\sum \limits_{i=1}^n y_i [x_i \text{ among k nearest neighbors of } x])$
%
$k_1(x,y) + k_2(x,y)$ , 
$k_1(x,y) \cdot k_2(x,y)$, 
$c \cdot k_1(x,y)$ for $c>0$ , 
$f(k_1(x,y))$, where $f$ is exponential/polynomial with positive coefficents\\
$k(x,y) = \phi(x)^T \phi(y)$, for some $\phi$ ass. with k

%\subsubsection*{Parametric vs. Nonparametric}
%\emph{Parametric}: have finite set of parameters\\
%E.g. linear regression, perceptron,...\\
%$f(x) = w^Tx, w\in \mathbb{R}^d$ (d is independent of #data)
% \begin{itemize}
% 	\item[+] computationally not complex
% \end{itemize}

%\emph{Nonparametric}: grows in complexity with the size of the data\\
%E.g. kernelized Perceptron, k-NN,...\\
%$f(x) = \sum_{i=1}^n \alpha_i y_i k(x_i,x_n)$ (depends on #data)\\
% \begin{itemize}
% 	\item[+] potentially much more expressive.
% \end{itemize}